input {
    kafka {
        id => "logstash plugin id"
        topics => ["kafka-topic-1", "kafka-topic-2"]
        bootstrap_servers => "kafka1:9092,kafka2:9092"
        codec => "json"
        group_id => "kafka_group"
    }
}
 
filter {
    json {
        source => "json_source"
        target => "json_target"
    }
    date {
        match => ['time', 'UNIX_MS', 'yyyy-MM-dd HH:mm:ss']
    }
    # for default geoip
    geoip {
        source => "ip"
    }
    # for additional data like isp,...
    geoip {
        source => "ip"
        database => "/path/to/GeoLite2-ASN.mmdb"
        default_database_type => "ASN"
    }
    useragent {
        source => "user_agent"
        target => "ua"
    }
    if "_jsonparsefailure" in [tags] {           
        mutate {
            add_field => { "some_field" => "unknown" }
        }
    }
    # generate unique id, use for exactly-one semantrics
    fingerprint {
        source => ["field1", "field1", "@timestamp"]
        target => "[@metadata][fingerprint]"
        method => "SHA1"
        key => "Log analytics"
        base64encode => true
    }
}
output {
    elasticsearch {
        hosts => ["elastic1:9200", "elastic2:9200"]
        index => "index_name"
        document_type => "%{some_field}"
        document_id => "%{[@metadata][fingerprint]}"
    }
    stdout {
        codec => rubydebug
    }
}